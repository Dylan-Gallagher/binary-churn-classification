{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5219d541-9440-4731-bcf5-8fde0f078905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data/train.csv').drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "X = pd.get_dummies(data.drop(['Exited'], axis=1), columns=['Geography', 'Gender'], dtype=float)\n",
    "y = data['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e29aa8-b1ec-46b9-820f-bf1ff0d628f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Normalize the data\n",
    "columns_to_normalize = [\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\", \"Tenure\"]\n",
    "scaler = StandardScaler()\n",
    "X[columns_to_normalize] = scaler.fit_transform(X[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0697aff-4c80-43da-9106-a6a89d248354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make train-dev split\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2)\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "X_test = pd.get_dummies(test_data.drop(['CustomerId', 'Surname'], axis=1), columns=['Geography', 'Gender'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6eca67-1e4a-4312-a39e-4af8807c56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "class ChurnDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ChurnDataset(X_train, y_train)\n",
    "dev_dataset = ChurnDataset(X_dev, y_dev)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73f69f03-b9bd-4b97-bc47-e94d4d0ee61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Make a neural network\n",
    "class NNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(13, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c14f47e1-cd4c-4534-bb2e-51f6eb35ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNet()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        pred = model(X).squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b94fa87d-e25f-4a97-83b8-7c224fc7f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X).squeeze()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((pred > 0.5) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30434c09-9642-41de-8bcb-d00e1e34c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.176218  [   32/132027]\n",
      "loss: 0.360422  [ 3232/132027]\n",
      "loss: 0.263328  [ 6432/132027]\n",
      "loss: 0.330421  [ 9632/132027]\n",
      "loss: 0.270047  [12832/132027]\n",
      "loss: 0.124210  [16032/132027]\n",
      "loss: 0.215223  [19232/132027]\n",
      "loss: 0.267102  [22432/132027]\n",
      "loss: 0.551481  [25632/132027]\n",
      "loss: 0.302512  [28832/132027]\n",
      "loss: 0.355732  [32032/132027]\n",
      "loss: 0.404689  [35232/132027]\n",
      "loss: 0.241599  [38432/132027]\n",
      "loss: 0.193409  [41632/132027]\n",
      "loss: 0.403787  [44832/132027]\n",
      "loss: 0.295977  [48032/132027]\n",
      "loss: 0.249863  [51232/132027]\n",
      "loss: 0.399807  [54432/132027]\n",
      "loss: 0.423681  [57632/132027]\n",
      "loss: 0.356889  [60832/132027]\n",
      "loss: 0.339417  [64032/132027]\n",
      "loss: 0.392047  [67232/132027]\n",
      "loss: 0.324805  [70432/132027]\n",
      "loss: 0.350771  [73632/132027]\n",
      "loss: 0.333274  [76832/132027]\n",
      "loss: 0.280882  [80032/132027]\n",
      "loss: 0.211277  [83232/132027]\n",
      "loss: 0.299580  [86432/132027]\n",
      "loss: 0.317337  [89632/132027]\n",
      "loss: 0.357279  [92832/132027]\n",
      "loss: 0.456736  [96032/132027]\n",
      "loss: 0.268429  [99232/132027]\n",
      "loss: 0.135960  [102432/132027]\n",
      "loss: 0.743540  [105632/132027]\n",
      "loss: 0.399298  [108832/132027]\n",
      "loss: 0.558018  [112032/132027]\n",
      "loss: 0.251296  [115232/132027]\n",
      "loss: 0.414953  [118432/132027]\n",
      "loss: 0.523286  [121632/132027]\n",
      "loss: 0.320369  [124832/132027]\n",
      "loss: 0.383924  [128032/132027]\n",
      "loss: 0.228101  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.340103 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.403998  [   32/132027]\n",
      "loss: 0.270257  [ 3232/132027]\n",
      "loss: 0.328480  [ 6432/132027]\n",
      "loss: 0.256594  [ 9632/132027]\n",
      "loss: 0.351480  [12832/132027]\n",
      "loss: 0.219599  [16032/132027]\n",
      "loss: 0.415460  [19232/132027]\n",
      "loss: 0.495774  [22432/132027]\n",
      "loss: 0.209636  [25632/132027]\n",
      "loss: 0.407448  [28832/132027]\n",
      "loss: 0.226466  [32032/132027]\n",
      "loss: 0.532954  [35232/132027]\n",
      "loss: 0.282396  [38432/132027]\n",
      "loss: 0.529478  [41632/132027]\n",
      "loss: 0.562051  [44832/132027]\n",
      "loss: 0.397514  [48032/132027]\n",
      "loss: 0.572090  [51232/132027]\n",
      "loss: 0.261287  [54432/132027]\n",
      "loss: 0.237130  [57632/132027]\n",
      "loss: 0.299590  [60832/132027]\n",
      "loss: 0.387935  [64032/132027]\n",
      "loss: 0.236049  [67232/132027]\n",
      "loss: 0.539487  [70432/132027]\n",
      "loss: 0.191368  [73632/132027]\n",
      "loss: 0.323912  [76832/132027]\n",
      "loss: 0.198529  [80032/132027]\n",
      "loss: 0.532052  [83232/132027]\n",
      "loss: 0.280305  [86432/132027]\n",
      "loss: 0.309888  [89632/132027]\n",
      "loss: 0.284286  [92832/132027]\n",
      "loss: 0.203630  [96032/132027]\n",
      "loss: 0.248641  [99232/132027]\n",
      "loss: 0.283163  [102432/132027]\n",
      "loss: 0.201436  [105632/132027]\n",
      "loss: 0.428476  [108832/132027]\n",
      "loss: 0.411618  [112032/132027]\n",
      "loss: 0.411549  [115232/132027]\n",
      "loss: 0.255633  [118432/132027]\n",
      "loss: 0.367423  [121632/132027]\n",
      "loss: 0.392647  [124832/132027]\n",
      "loss: 0.416247  [128032/132027]\n",
      "loss: 0.209476  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.330922 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.501033  [   32/132027]\n",
      "loss: 0.472472  [ 3232/132027]\n",
      "loss: 0.391131  [ 6432/132027]\n",
      "loss: 0.330377  [ 9632/132027]\n",
      "loss: 0.240649  [12832/132027]\n",
      "loss: 0.235038  [16032/132027]\n",
      "loss: 0.623181  [19232/132027]\n",
      "loss: 0.403431  [22432/132027]\n",
      "loss: 0.231955  [25632/132027]\n",
      "loss: 0.300967  [28832/132027]\n",
      "loss: 0.400976  [32032/132027]\n",
      "loss: 0.358082  [35232/132027]\n",
      "loss: 0.353088  [38432/132027]\n",
      "loss: 0.200999  [41632/132027]\n",
      "loss: 0.433162  [44832/132027]\n",
      "loss: 0.437138  [48032/132027]\n",
      "loss: 0.294545  [51232/132027]\n",
      "loss: 0.578088  [54432/132027]\n",
      "loss: 0.483734  [57632/132027]\n",
      "loss: 0.366493  [60832/132027]\n",
      "loss: 0.543259  [64032/132027]\n",
      "loss: 0.289033  [67232/132027]\n",
      "loss: 0.322529  [70432/132027]\n",
      "loss: 0.308249  [73632/132027]\n",
      "loss: 0.410768  [76832/132027]\n",
      "loss: 0.269291  [80032/132027]\n",
      "loss: 0.395219  [83232/132027]\n",
      "loss: 0.395237  [86432/132027]\n",
      "loss: 0.336590  [89632/132027]\n",
      "loss: 0.193313  [92832/132027]\n",
      "loss: 0.272936  [96032/132027]\n",
      "loss: 0.196692  [99232/132027]\n",
      "loss: 0.334203  [102432/132027]\n",
      "loss: 0.469408  [105632/132027]\n",
      "loss: 0.507226  [108832/132027]\n",
      "loss: 0.269117  [112032/132027]\n",
      "loss: 0.347673  [115232/132027]\n",
      "loss: 0.229867  [118432/132027]\n",
      "loss: 0.345924  [121632/132027]\n",
      "loss: 0.491607  [124832/132027]\n",
      "loss: 0.268568  [128032/132027]\n",
      "loss: 0.413204  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.326251 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.213410  [   32/132027]\n",
      "loss: 0.248166  [ 3232/132027]\n",
      "loss: 0.236271  [ 6432/132027]\n",
      "loss: 0.515454  [ 9632/132027]\n",
      "loss: 0.250623  [12832/132027]\n",
      "loss: 0.183000  [16032/132027]\n",
      "loss: 0.103385  [19232/132027]\n",
      "loss: 0.369617  [22432/132027]\n",
      "loss: 0.159429  [25632/132027]\n",
      "loss: 0.506355  [28832/132027]\n",
      "loss: 0.342086  [32032/132027]\n",
      "loss: 0.198645  [35232/132027]\n",
      "loss: 0.465438  [38432/132027]\n",
      "loss: 0.434835  [41632/132027]\n",
      "loss: 0.275453  [44832/132027]\n",
      "loss: 0.364279  [48032/132027]\n",
      "loss: 0.279382  [51232/132027]\n",
      "loss: 0.548548  [54432/132027]\n",
      "loss: 0.372684  [57632/132027]\n",
      "loss: 0.328996  [60832/132027]\n",
      "loss: 0.354178  [64032/132027]\n",
      "loss: 0.141555  [67232/132027]\n",
      "loss: 0.387223  [70432/132027]\n",
      "loss: 0.283636  [73632/132027]\n",
      "loss: 0.337576  [76832/132027]\n",
      "loss: 0.378935  [80032/132027]\n",
      "loss: 0.215801  [83232/132027]\n",
      "loss: 0.744455  [86432/132027]\n",
      "loss: 0.277986  [89632/132027]\n",
      "loss: 0.223936  [92832/132027]\n",
      "loss: 0.225930  [96032/132027]\n",
      "loss: 0.197042  [99232/132027]\n",
      "loss: 0.164043  [102432/132027]\n",
      "loss: 0.435632  [105632/132027]\n",
      "loss: 0.380534  [108832/132027]\n",
      "loss: 0.318944  [112032/132027]\n",
      "loss: 0.615797  [115232/132027]\n",
      "loss: 0.178306  [118432/132027]\n",
      "loss: 0.140851  [121632/132027]\n",
      "loss: 0.289691  [124832/132027]\n",
      "loss: 0.272001  [128032/132027]\n",
      "loss: 0.365127  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.334066 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.419025  [   32/132027]\n",
      "loss: 0.418433  [ 3232/132027]\n",
      "loss: 0.360423  [ 6432/132027]\n",
      "loss: 0.310866  [ 9632/132027]\n",
      "loss: 0.245223  [12832/132027]\n",
      "loss: 0.315805  [16032/132027]\n",
      "loss: 0.292010  [19232/132027]\n",
      "loss: 0.240987  [22432/132027]\n",
      "loss: 0.609828  [25632/132027]\n",
      "loss: 0.690115  [28832/132027]\n",
      "loss: 0.502137  [32032/132027]\n",
      "loss: 0.327776  [35232/132027]\n",
      "loss: 0.172551  [38432/132027]\n",
      "loss: 0.371160  [41632/132027]\n",
      "loss: 0.183777  [44832/132027]\n",
      "loss: 0.196546  [48032/132027]\n",
      "loss: 0.250584  [51232/132027]\n",
      "loss: 0.460582  [54432/132027]\n",
      "loss: 0.365191  [57632/132027]\n",
      "loss: 0.331820  [60832/132027]\n",
      "loss: 0.279271  [64032/132027]\n",
      "loss: 0.240465  [67232/132027]\n",
      "loss: 0.305256  [70432/132027]\n",
      "loss: 0.269661  [73632/132027]\n",
      "loss: 0.345176  [76832/132027]\n",
      "loss: 0.339790  [80032/132027]\n",
      "loss: 0.494803  [83232/132027]\n",
      "loss: 0.137222  [86432/132027]\n",
      "loss: 0.444219  [89632/132027]\n",
      "loss: 0.304327  [92832/132027]\n",
      "loss: 0.188880  [96032/132027]\n",
      "loss: 0.226592  [99232/132027]\n",
      "loss: 0.102527  [102432/132027]\n",
      "loss: 0.253270  [105632/132027]\n",
      "loss: 0.481229  [108832/132027]\n",
      "loss: 0.267196  [112032/132027]\n",
      "loss: 0.428666  [115232/132027]\n",
      "loss: 0.339916  [118432/132027]\n",
      "loss: 0.322623  [121632/132027]\n",
      "loss: 0.303009  [124832/132027]\n",
      "loss: 0.321593  [128032/132027]\n",
      "loss: 0.392791  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.331181 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.610845  [   32/132027]\n",
      "loss: 0.371722  [ 3232/132027]\n",
      "loss: 0.359974  [ 6432/132027]\n",
      "loss: 0.448657  [ 9632/132027]\n",
      "loss: 0.379744  [12832/132027]\n",
      "loss: 0.379814  [16032/132027]\n",
      "loss: 0.280243  [19232/132027]\n",
      "loss: 0.460034  [22432/132027]\n",
      "loss: 0.187909  [25632/132027]\n",
      "loss: 0.529064  [28832/132027]\n",
      "loss: 0.276015  [32032/132027]\n",
      "loss: 0.309130  [35232/132027]\n",
      "loss: 0.186176  [38432/132027]\n",
      "loss: 0.270217  [41632/132027]\n",
      "loss: 0.395817  [44832/132027]\n",
      "loss: 0.153987  [48032/132027]\n",
      "loss: 0.326776  [51232/132027]\n",
      "loss: 0.152265  [54432/132027]\n",
      "loss: 0.207725  [57632/132027]\n",
      "loss: 0.203418  [60832/132027]\n",
      "loss: 0.288903  [64032/132027]\n",
      "loss: 0.309129  [67232/132027]\n",
      "loss: 0.247352  [70432/132027]\n",
      "loss: 0.191531  [73632/132027]\n",
      "loss: 0.266025  [76832/132027]\n",
      "loss: 0.274713  [80032/132027]\n",
      "loss: 0.444086  [83232/132027]\n",
      "loss: 0.331683  [86432/132027]\n",
      "loss: 0.217838  [89632/132027]\n",
      "loss: 0.162513  [92832/132027]\n",
      "loss: 0.389620  [96032/132027]\n",
      "loss: 0.249739  [99232/132027]\n",
      "loss: 0.737215  [102432/132027]\n",
      "loss: 0.591373  [105632/132027]\n",
      "loss: 0.155334  [108832/132027]\n",
      "loss: 0.189287  [112032/132027]\n",
      "loss: 0.654329  [115232/132027]\n",
      "loss: 0.269392  [118432/132027]\n",
      "loss: 0.241924  [121632/132027]\n",
      "loss: 0.419513  [124832/132027]\n",
      "loss: 0.204058  [128032/132027]\n",
      "loss: 0.476543  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.453970 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.300405  [   32/132027]\n",
      "loss: 0.291143  [ 3232/132027]\n",
      "loss: 0.194677  [ 6432/132027]\n",
      "loss: 0.529709  [ 9632/132027]\n",
      "loss: 0.438411  [12832/132027]\n",
      "loss: 0.391625  [16032/132027]\n",
      "loss: 0.451573  [19232/132027]\n",
      "loss: 0.410109  [22432/132027]\n",
      "loss: 0.287292  [25632/132027]\n",
      "loss: 0.372467  [28832/132027]\n",
      "loss: 0.848733  [32032/132027]\n",
      "loss: 0.424576  [35232/132027]\n",
      "loss: 0.370827  [38432/132027]\n",
      "loss: 0.344293  [41632/132027]\n",
      "loss: 0.252370  [44832/132027]\n",
      "loss: 0.311611  [48032/132027]\n",
      "loss: 0.189079  [51232/132027]\n",
      "loss: 0.141928  [54432/132027]\n",
      "loss: 0.512080  [57632/132027]\n",
      "loss: 0.173925  [60832/132027]\n",
      "loss: 0.370830  [64032/132027]\n",
      "loss: 0.199055  [67232/132027]\n",
      "loss: 0.279039  [70432/132027]\n",
      "loss: 0.229789  [73632/132027]\n",
      "loss: 0.343549  [76832/132027]\n",
      "loss: 0.514835  [80032/132027]\n",
      "loss: 0.337508  [83232/132027]\n",
      "loss: 0.327646  [86432/132027]\n",
      "loss: 0.284416  [89632/132027]\n",
      "loss: 0.182148  [92832/132027]\n",
      "loss: 0.194358  [96032/132027]\n",
      "loss: 0.301838  [99232/132027]\n",
      "loss: 0.297057  [102432/132027]\n",
      "loss: 0.331622  [105632/132027]\n",
      "loss: 0.301440  [108832/132027]\n",
      "loss: 0.420931  [112032/132027]\n",
      "loss: 0.479928  [115232/132027]\n",
      "loss: 0.244499  [118432/132027]\n",
      "loss: 0.351992  [121632/132027]\n",
      "loss: 0.317027  [124832/132027]\n",
      "loss: 0.234406  [128032/132027]\n",
      "loss: 0.308338  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.324608 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.413071  [   32/132027]\n",
      "loss: 0.242789  [ 3232/132027]\n",
      "loss: 0.431645  [ 6432/132027]\n",
      "loss: 0.320499  [ 9632/132027]\n",
      "loss: 0.378287  [12832/132027]\n",
      "loss: 0.307209  [16032/132027]\n",
      "loss: 0.295250  [19232/132027]\n",
      "loss: 0.278899  [22432/132027]\n",
      "loss: 0.483068  [25632/132027]\n",
      "loss: 0.302408  [28832/132027]\n",
      "loss: 0.340353  [32032/132027]\n",
      "loss: 0.314898  [35232/132027]\n",
      "loss: 0.359104  [38432/132027]\n",
      "loss: 0.371959  [41632/132027]\n",
      "loss: 0.417006  [44832/132027]\n",
      "loss: 0.327061  [48032/132027]\n",
      "loss: 0.314577  [51232/132027]\n",
      "loss: 0.406921  [54432/132027]\n",
      "loss: 0.391300  [57632/132027]\n",
      "loss: 0.264701  [60832/132027]\n",
      "loss: 0.414856  [64032/132027]\n",
      "loss: 0.199148  [67232/132027]\n",
      "loss: 0.199029  [70432/132027]\n",
      "loss: 0.177166  [73632/132027]\n",
      "loss: 0.427391  [76832/132027]\n",
      "loss: 0.508641  [80032/132027]\n",
      "loss: 0.272910  [83232/132027]\n",
      "loss: 0.520039  [86432/132027]\n",
      "loss: 0.324923  [89632/132027]\n",
      "loss: 0.223855  [92832/132027]\n",
      "loss: 0.386358  [96032/132027]\n",
      "loss: 0.347159  [99232/132027]\n",
      "loss: 0.481305  [102432/132027]\n",
      "loss: 0.324439  [105632/132027]\n",
      "loss: 0.304190  [108832/132027]\n",
      "loss: 0.319076  [112032/132027]\n",
      "loss: 0.332582  [115232/132027]\n",
      "loss: 0.522736  [118432/132027]\n",
      "loss: 0.288724  [121632/132027]\n",
      "loss: 0.274034  [124832/132027]\n",
      "loss: 0.283492  [128032/132027]\n",
      "loss: 0.439990  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.327898 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.420018  [   32/132027]\n",
      "loss: 0.261630  [ 3232/132027]\n",
      "loss: 0.362260  [ 6432/132027]\n",
      "loss: 0.398863  [ 9632/132027]\n",
      "loss: 0.373009  [12832/132027]\n",
      "loss: 0.361930  [16032/132027]\n",
      "loss: 0.388582  [19232/132027]\n",
      "loss: 0.277967  [22432/132027]\n",
      "loss: 0.396352  [25632/132027]\n",
      "loss: 0.546482  [28832/132027]\n",
      "loss: 0.402292  [32032/132027]\n",
      "loss: 0.231300  [35232/132027]\n",
      "loss: 0.191366  [38432/132027]\n",
      "loss: 0.362720  [41632/132027]\n",
      "loss: 0.367429  [44832/132027]\n",
      "loss: 0.399668  [48032/132027]\n",
      "loss: 0.518993  [51232/132027]\n",
      "loss: 0.334556  [54432/132027]\n",
      "loss: 0.395322  [57632/132027]\n",
      "loss: 0.551287  [60832/132027]\n",
      "loss: 0.565582  [64032/132027]\n",
      "loss: 0.416760  [67232/132027]\n",
      "loss: 0.384828  [70432/132027]\n",
      "loss: 0.170597  [73632/132027]\n",
      "loss: 0.311311  [76832/132027]\n",
      "loss: 0.439236  [80032/132027]\n",
      "loss: 0.524710  [83232/132027]\n",
      "loss: 0.262189  [86432/132027]\n",
      "loss: 0.310093  [89632/132027]\n",
      "loss: 0.350032  [92832/132027]\n",
      "loss: 0.483610  [96032/132027]\n",
      "loss: 0.403318  [99232/132027]\n",
      "loss: 0.249115  [102432/132027]\n",
      "loss: 0.231752  [105632/132027]\n",
      "loss: 0.195661  [108832/132027]\n",
      "loss: 0.515382  [112032/132027]\n",
      "loss: 0.463044  [115232/132027]\n",
      "loss: 0.435020  [118432/132027]\n",
      "loss: 0.332943  [121632/132027]\n",
      "loss: 0.296791  [124832/132027]\n",
      "loss: 0.494270  [128032/132027]\n",
      "loss: 0.483268  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.326626 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.178256  [   32/132027]\n",
      "loss: 0.342729  [ 3232/132027]\n",
      "loss: 0.343810  [ 6432/132027]\n",
      "loss: 0.281777  [ 9632/132027]\n",
      "loss: 0.329756  [12832/132027]\n",
      "loss: 0.331660  [16032/132027]\n",
      "loss: 0.381230  [19232/132027]\n",
      "loss: 0.253203  [22432/132027]\n",
      "loss: 0.240157  [25632/132027]\n",
      "loss: 0.233997  [28832/132027]\n",
      "loss: 0.502996  [32032/132027]\n",
      "loss: 0.323890  [35232/132027]\n",
      "loss: 0.332262  [38432/132027]\n",
      "loss: 0.422749  [41632/132027]\n",
      "loss: 0.509667  [44832/132027]\n",
      "loss: 0.415791  [48032/132027]\n",
      "loss: 0.410101  [51232/132027]\n",
      "loss: 0.177344  [54432/132027]\n",
      "loss: 0.162424  [57632/132027]\n",
      "loss: 0.442532  [60832/132027]\n",
      "loss: 0.174899  [64032/132027]\n",
      "loss: 0.225439  [67232/132027]\n",
      "loss: 0.308369  [70432/132027]\n",
      "loss: 0.290882  [73632/132027]\n",
      "loss: 0.404560  [76832/132027]\n",
      "loss: 0.247968  [80032/132027]\n",
      "loss: 0.386877  [83232/132027]\n",
      "loss: 0.397290  [86432/132027]\n",
      "loss: 0.186133  [89632/132027]\n",
      "loss: 0.461063  [92832/132027]\n",
      "loss: 0.251757  [96032/132027]\n",
      "loss: 0.410852  [99232/132027]\n",
      "loss: 0.295091  [102432/132027]\n",
      "loss: 0.277078  [105632/132027]\n",
      "loss: 0.419959  [108832/132027]\n",
      "loss: 0.294905  [112032/132027]\n",
      "loss: 0.564739  [115232/132027]\n",
      "loss: 0.309638  [118432/132027]\n",
      "loss: 0.274574  [121632/132027]\n",
      "loss: 0.193833  [124832/132027]\n",
      "loss: 0.346196  [128032/132027]\n",
      "loss: 0.267204  [131232/132027]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.325231 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(dev_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e67ef-7067-4778-ae23-fbb4ac93dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a50f8e-eb0c-43c8-b52b-e3f82ae598c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
